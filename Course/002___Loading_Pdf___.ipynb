{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8aeab9",
   "metadata": {},
   "source": [
    "# __Loading Pdf Files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf5f8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce83176",
   "metadata": {},
   "source": [
    "# `PyPDFLoader()`\n",
    "\n",
    "It loads __every page in a separate Document__ with a lot of metadata\n",
    "\n",
    "- Simple and reliable\n",
    "- Good for most PDFs\n",
    "- Preserves page numbers\n",
    "- __ONLY Basic Text Extraction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3295548e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 documents with PyPDFLoader.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pypdf_loader = PyPDFLoader(\"data/pdfs/attention.pdf\")\n",
    "    pypdf_docs = pypdf_loader.load()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PDF with PyPDFLoader: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(pypdf_docs)} documents with PyPDFLoader.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd2c9fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 1 ---\n",
      "________________________________________________________________________________\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brai ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: PyPDF\n",
      "  creationdate: \n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  publisher: Curran Associates, Inc.\n",
      "  language: en-US\n",
      "  created: 2017\n",
      "  eventtype: Poster\n",
      "  description-abstract: The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.\n",
      "  title: Attention is All you Need\n",
      "  date: 2017\n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  published: 2017\n",
      "  type: Conference Proceedings\n",
      "  firstpage: 5998\n",
      "  book: Advances in Neural Information Processing Systems 30\n",
      "  description: Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)\n",
      "  editors: I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  lastpage: 6008\n",
      "  source: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  page: 0\n",
      "  page_label: 1\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 2 ---\n",
      "________________________________________________________________________________\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "seq ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: PyPDF\n",
      "  creationdate: \n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  publisher: Curran Associates, Inc.\n",
      "  language: en-US\n",
      "  created: 2017\n",
      "  eventtype: Poster\n",
      "  description-abstract: The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.\n",
      "  title: Attention is All you Need\n",
      "  date: 2017\n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  published: 2017\n",
      "  type: Conference Proceedings\n",
      "  firstpage: 5998\n",
      "  book: Advances in Neural Information Processing Systems 30\n",
      "  description: Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)\n",
      "  editors: I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  lastpage: 6008\n",
      "  source: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  page: 1\n",
      "  page_label: 2\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 3 ---\n",
      "________________________________________________________________________________\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: PyPDF\n",
      "  creationdate: \n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  publisher: Curran Associates, Inc.\n",
      "  language: en-US\n",
      "  created: 2017\n",
      "  eventtype: Poster\n",
      "  description-abstract: The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.\n",
      "  title: Attention is All you Need\n",
      "  date: 2017\n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  published: 2017\n",
      "  type: Conference Proceedings\n",
      "  firstpage: 5998\n",
      "  book: Advances in Neural Information Processing Systems 30\n",
      "  description: Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)\n",
      "  editors: I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  lastpage: 6008\n",
      "  source: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  page: 2\n",
      "  page_label: 3\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 4 ---\n",
      "________________________________________________________________________________\n",
      "Scaled Dot-Product Attention\n",
      " Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (r ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: PyPDF\n",
      "  creationdate: \n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  publisher: Curran Associates, Inc.\n",
      "  language: en-US\n",
      "  created: 2017\n",
      "  eventtype: Poster\n",
      "  description-abstract: The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.\n",
      "  title: Attention is All you Need\n",
      "  date: 2017\n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  published: 2017\n",
      "  type: Conference Proceedings\n",
      "  firstpage: 5998\n",
      "  book: Advances in Neural Information Processing Systems 30\n",
      "  description: Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)\n",
      "  editors: I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  lastpage: 6008\n",
      "  source: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  page: 3\n",
      "  page_label: 4\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 5 ---\n",
      "________________________________________________________________________________\n",
      "MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\n",
      "where headi = Attention(QWQ\n",
      "i ,KW K\n",
      "i ,VW V\n",
      "i )\n",
      "Where ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: PyPDF\n",
      "  creationdate: \n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  publisher: Curran Associates, Inc.\n",
      "  language: en-US\n",
      "  created: 2017\n",
      "  eventtype: Poster\n",
      "  description-abstract: The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.\n",
      "  title: Attention is All you Need\n",
      "  date: 2017\n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  published: 2017\n",
      "  type: Conference Proceedings\n",
      "  firstpage: 5998\n",
      "  book: Advances in Neural Information Processing Systems 30\n",
      "  description: Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)\n",
      "  editors: I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  lastpage: 6008\n",
      "  source: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  page: 4\n",
      "  page_label: 5\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 6 ---\n",
      "________________________________________________________________________________\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for  ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: PyPDF\n",
      "  creationdate: \n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  publisher: Curran Associates, Inc.\n",
      "  language: en-US\n",
      "  created: 2017\n",
      "  eventtype: Poster\n",
      "  description-abstract: The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.\n",
      "  title: Attention is All you Need\n",
      "  date: 2017\n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  published: 2017\n",
      "  type: Conference Proceedings\n",
      "  firstpage: 5998\n",
      "  book: Advances in Neural Information Processing Systems 30\n",
      "  description: Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)\n",
      "  editors: I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  lastpage: 6008\n",
      "  source: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  page: 5\n",
      "  page_label: 6\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 7 ---\n",
      "________________________________________________________________________________\n",
      "the input sequence centered around the respective output position. This would increase the maximum\n",
      "p ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: PyPDF\n",
      "  creationdate: \n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  publisher: Curran Associates, Inc.\n",
      "  language: en-US\n",
      "  created: 2017\n",
      "  eventtype: Poster\n",
      "  description-abstract: The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.\n",
      "  title: Attention is All you Need\n",
      "  date: 2017\n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  published: 2017\n",
      "  type: Conference Proceedings\n",
      "  firstpage: 5998\n",
      "  book: Advances in Neural Information Processing Systems 30\n",
      "  description: Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)\n",
      "  editors: I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  lastpage: 6008\n",
      "  source: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  page: 6\n",
      "  page_label: 7\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 8 ---\n",
      "________________________________________________________________________________\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "En ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: PyPDF\n",
      "  creationdate: \n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  publisher: Curran Associates, Inc.\n",
      "  language: en-US\n",
      "  created: 2017\n",
      "  eventtype: Poster\n",
      "  description-abstract: The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.\n",
      "  title: Attention is All you Need\n",
      "  date: 2017\n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  published: 2017\n",
      "  type: Conference Proceedings\n",
      "  firstpage: 5998\n",
      "  book: Advances in Neural Information Processing Systems 30\n",
      "  description: Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)\n",
      "  editors: I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  lastpage: 6008\n",
      "  source: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  page: 7\n",
      "  page_label: 8\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 9 ---\n",
      "________________________________________________________________________________\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the b ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: PyPDF\n",
      "  creationdate: \n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  publisher: Curran Associates, Inc.\n",
      "  language: en-US\n",
      "  created: 2017\n",
      "  eventtype: Poster\n",
      "  description-abstract: The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.\n",
      "  title: Attention is All you Need\n",
      "  date: 2017\n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  published: 2017\n",
      "  type: Conference Proceedings\n",
      "  firstpage: 5998\n",
      "  book: Advances in Neural Information Processing Systems 30\n",
      "  description: Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)\n",
      "  editors: I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  lastpage: 6008\n",
      "  source: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  page: 8\n",
      "  page_label: 9\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 10 ---\n",
      "________________________________________________________________________________\n",
      "References\n",
      "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv pre ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: PyPDF\n",
      "  creationdate: \n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  publisher: Curran Associates, Inc.\n",
      "  language: en-US\n",
      "  created: 2017\n",
      "  eventtype: Poster\n",
      "  description-abstract: The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.\n",
      "  title: Attention is All you Need\n",
      "  date: 2017\n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  published: 2017\n",
      "  type: Conference Proceedings\n",
      "  firstpage: 5998\n",
      "  book: Advances in Neural Information Processing Systems 30\n",
      "  description: Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)\n",
      "  editors: I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  lastpage: 6008\n",
      "  source: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  page: 9\n",
      "  page_label: 10\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 11 ---\n",
      "________________________________________________________________________________\n",
      "[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "base ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: PyPDF\n",
      "  creationdate: \n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  publisher: Curran Associates, Inc.\n",
      "  language: en-US\n",
      "  created: 2017\n",
      "  eventtype: Poster\n",
      "  description-abstract: The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.\n",
      "  title: Attention is All you Need\n",
      "  date: 2017\n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  published: 2017\n",
      "  type: Conference Proceedings\n",
      "  firstpage: 5998\n",
      "  book: Advances in Neural Information Processing Systems 30\n",
      "  description: Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)\n",
      "  editors: I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  lastpage: 6008\n",
      "  source: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  page: 10\n",
      "  page_label: 11\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(pypdf_docs):\n",
    "    print(\"_\" * 80)\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(\"_\" * 80)\n",
    "    print(doc.page_content[:100], '...')\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b375750",
   "metadata": {},
   "source": [
    "# `PyMuPDFLoader()`\n",
    "\n",
    "- Fast\n",
    "- Good text extraction\n",
    "- __Supports Image Extraction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3566e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11 documents with PyMuPDFLoader.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pymupdf_loader = PyMuPDFLoader(\"data/pdfs/attention.pdf\")\n",
    "    pymupdf_docs = pymupdf_loader.load()\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PDF with PyPDFLoader: {e}\")\n",
    "\n",
    "print(f\"Loaded {len(pymupdf_docs)} documents with PyMuPDFLoader.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b3b0d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 1 ---\n",
      "________________________________________________________________________________\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brai ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: \n",
      "  creationdate: \n",
      "  source: data/pdfs/attention.pdf\n",
      "  file_path: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  format: PDF 1.3\n",
      "  title: Attention is All you Need\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  keywords: \n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  trapped: \n",
      "  modDate: D:20180212212210-08'00'\n",
      "  creationDate: \n",
      "  page: 0\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 2 ---\n",
      "________________________________________________________________________________\n",
      "Recurrent models typically factor computation along the symbol positions of the input and output\n",
      "seq ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: \n",
      "  creationdate: \n",
      "  source: data/pdfs/attention.pdf\n",
      "  file_path: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  format: PDF 1.3\n",
      "  title: Attention is All you Need\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  keywords: \n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  trapped: \n",
      "  modDate: D:20180212212210-08'00'\n",
      "  creationDate: \n",
      "  page: 1\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 3 ---\n",
      "________________________________________________________________________________\n",
      "Figure 1: The Transformer - model architecture.\n",
      "wise fully connected feed-forward network. We employ ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: \n",
      "  creationdate: \n",
      "  source: data/pdfs/attention.pdf\n",
      "  file_path: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  format: PDF 1.3\n",
      "  title: Attention is All you Need\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  keywords: \n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  trapped: \n",
      "  modDate: D:20180212212210-08'00'\n",
      "  creationDate: \n",
      "  page: 2\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 4 ---\n",
      "________________________________________________________________________________\n",
      "Scaled Dot-Product Attention\n",
      "Multi-Head Attention\n",
      "Figure 2: (left) Scaled Dot-Product Attention. (ri ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: \n",
      "  creationdate: \n",
      "  source: data/pdfs/attention.pdf\n",
      "  file_path: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  format: PDF 1.3\n",
      "  title: Attention is All you Need\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  keywords: \n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  trapped: \n",
      "  modDate: D:20180212212210-08'00'\n",
      "  creationDate: \n",
      "  page: 3\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 5 ---\n",
      "________________________________________________________________________________\n",
      "MultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\n",
      "where headi = Attention(QW Q\n",
      "i , KW K\n",
      "i , V W V\n",
      "i ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: \n",
      "  creationdate: \n",
      "  source: data/pdfs/attention.pdf\n",
      "  file_path: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  format: PDF 1.3\n",
      "  title: Attention is All you Need\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  keywords: \n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  trapped: \n",
      "  modDate: D:20180212212210-08'00'\n",
      "  creationDate: \n",
      "  page: 4\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 6 ---\n",
      "________________________________________________________________________________\n",
      "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\n",
      "for  ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: \n",
      "  creationdate: \n",
      "  source: data/pdfs/attention.pdf\n",
      "  file_path: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  format: PDF 1.3\n",
      "  title: Attention is All you Need\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  keywords: \n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  trapped: \n",
      "  modDate: D:20180212212210-08'00'\n",
      "  creationDate: \n",
      "  page: 5\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 7 ---\n",
      "________________________________________________________________________________\n",
      "the input sequence centered around the respective output position. This would increase the maximum\n",
      "p ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: \n",
      "  creationdate: \n",
      "  source: data/pdfs/attention.pdf\n",
      "  file_path: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  format: PDF 1.3\n",
      "  title: Attention is All you Need\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  keywords: \n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  trapped: \n",
      "  modDate: D:20180212212210-08'00'\n",
      "  creationDate: \n",
      "  page: 6\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 8 ---\n",
      "________________________________________________________________________________\n",
      "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\n",
      "En ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: \n",
      "  creationdate: \n",
      "  source: data/pdfs/attention.pdf\n",
      "  file_path: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  format: PDF 1.3\n",
      "  title: Attention is All you Need\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  keywords: \n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  trapped: \n",
      "  modDate: D:20180212212210-08'00'\n",
      "  creationDate: \n",
      "  page: 7\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 9 ---\n",
      "________________________________________________________________________________\n",
      "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the b ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: \n",
      "  creationdate: \n",
      "  source: data/pdfs/attention.pdf\n",
      "  file_path: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  format: PDF 1.3\n",
      "  title: Attention is All you Need\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  keywords: \n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  trapped: \n",
      "  modDate: D:20180212212210-08'00'\n",
      "  creationDate: \n",
      "  page: 8\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 10 ---\n",
      "________________________________________________________________________________\n",
      "References\n",
      "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv pre ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: \n",
      "  creationdate: \n",
      "  source: data/pdfs/attention.pdf\n",
      "  file_path: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  format: PDF 1.3\n",
      "  title: Attention is All you Need\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  keywords: \n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  trapped: \n",
      "  modDate: D:20180212212210-08'00'\n",
      "  creationDate: \n",
      "  page: 9\n",
      "________________________________________________________________________________\n",
      "\n",
      "--- Document 11 ---\n",
      "________________________________________________________________________________\n",
      "[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\n",
      "base ...\n",
      "Metadata:\n",
      "  producer: PyPDF2\n",
      "  creator: \n",
      "  creationdate: \n",
      "  source: data/pdfs/attention.pdf\n",
      "  file_path: data/pdfs/attention.pdf\n",
      "  total_pages: 11\n",
      "  format: PDF 1.3\n",
      "  title: Attention is All you Need\n",
      "  author: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin\n",
      "  subject: Neural Information Processing Systems http://nips.cc/\n",
      "  keywords: \n",
      "  moddate: 2018-02-12T21:22:10-08:00\n",
      "  trapped: \n",
      "  modDate: D:20180212212210-08'00'\n",
      "  creationDate: \n",
      "  page: 10\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(pymupdf_docs):\n",
    "    print(\"_\" * 80)\n",
    "    print(f\"\\n--- Document {i+1} ---\")\n",
    "    print(\"_\" * 80)\n",
    "    print(doc.page_content[:100], '...')\n",
    "    print(\"Metadata:\")\n",
    "    for key, value in doc.metadata.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820ff29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
